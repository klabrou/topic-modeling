{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Topic Modeler\n",
        "Python scripts for extracting text from PDF and TXT files, followed by text preprocessing using NLP methods and topic modeling of the preprocessed texts. Topic modeling can be used to perform preliminary data analysis to identify key themes and topics present in a corpus of text.\n",
        "Implementation adapted from [tutorial](https://towardsdatascience.com/topic-modeling-with-bert-779f7db187e6) by Maarten Grootendorst.\n",
        "\n",
        "Source: [Katerina Labrou](https://github.com/klabrou/topic-modeling). MIT License.\n",
        "\n"
      ],
      "metadata": {
        "id": "SI6IPGWOOzBH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "LdLYFFdzk9tt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0QVU2VqNCrJ"
      },
      "outputs": [],
      "source": [
        "import streamlit as st\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import csv\n",
        "from PyPDF2 import PdfReader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_pdfs(directory):\n",
        "    # Create a CSV file for storing the extracted data\n",
        "    output_csv = \"pdf_data.csv\"\n",
        "\n",
        "    # Open the CSV file in write mode\n",
        "    with open(output_csv, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        writer.writerow([\"Author\", \"Title\", \"Subject\", \"Content\"])\n",
        "\n",
        "\n",
        "\n",
        "        # Iterate through each PDF file in the directory\n",
        "        for filename in os.listdir(directory):\n",
        "            print(filename)\n",
        "            if filename.endswith(\".pdf\"):\n",
        "                pdf_path = os.path.join(directory, filename)\n",
        "\n",
        "                # Open the PDF file\n",
        "                with open(pdf_path, \"rb\") as pdf_file:\n",
        "                    try:\n",
        "                        pdf = PdfReader(pdf_file)\n",
        "\n",
        "                        # Extract metadata\n",
        "                        author = pdf.metadata.author\n",
        "                        title = pdf.metadata.title\n",
        "                        subject = pdf.metadata.subject\n",
        "\n",
        "                        # Extract content from each page\n",
        "                        content = \"\"\n",
        "                        for page_number in range(len(pdf.pages)):\n",
        "                            content += pdf.pages[page_number].extract_text()\n",
        "\n",
        "                        # Write the extracted data to the CSV file\n",
        "                        writer.writerow([author, title, subject, content])\n",
        "\n",
        "                        print(f\"Processed {filename}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error processing {filename}: {str(e)}\")\n",
        "\n",
        "            if filename.endswith(\".txt\"):\n",
        "                txt_path = os.path.join(directory, filename)\n",
        "\n",
        "                # Open the PDF file\n",
        "                with open(txt_path, \"rb\") as txt_file:\n",
        "\n",
        "                        content = txt_file.read()\n",
        "\n",
        "                        # Extract metadata\n",
        "                        author = \"\"\n",
        "                        title = \"\"\n",
        "                        subject = \" \"\n",
        "\n",
        "                        # Write the extracted data to the CSV file\n",
        "                        writer.writerow([author, title, subject, content])\n",
        "\n",
        "    print(\"Extraction complete. CSV file generated.\")\n",
        "\n",
        "directory_path = \"./\"\n",
        "process_pdfs(directory_path)"
      ],
      "metadata": {
        "id": "hNfHKhaGlcel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('./pdf_data.csv')"
      ],
      "metadata": {
        "id": "pksxkrGnmF8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove stopwords\n",
        "import nltk # https://medium.com/grabngoinfo/topic-modeling-with-deep-learning-using-python-bertopic-cf91f5676504\n",
        "nltk.download('stopwords')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('wordnet')\n",
        "wn = nltk.WordNetLemmatizer()\n",
        "\n",
        "# Remove stopwords\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "df['Content_without_stopwords'] = df['Content'].apply(lambda x: ' '.join([w for w in x.split() if w.lower() not in stopwords]))\n",
        "# Lemmatization\n",
        "df['Content_lemmatized'] = df['Content_without_stopwords'].apply(lambda x: ' '.join([wn.lemmatize(w) for w in x.split() if w not in stopwords]))"
      ],
      "metadata": {
        "id": "3sdf_lWcQfBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BHM92Bx4NCrM"
      },
      "outputs": [],
      "source": [
        "# Topic model\n",
        "from bertopic import BERTopic\n",
        "# Dimension reduction\n",
        "from umap import UMAP\n",
        "\n",
        "# Initiate UMAP\n",
        "umap_model = UMAP(n_neighbors= 5,\n",
        "                  n_components= 5,\n",
        "                  min_dist=0.0,\n",
        "                  metric='cosine',\n",
        "                  random_state=100)\n",
        "\n",
        "topic_model = BERTopic(umap_model=umap_model, language=\"english\", calculate_probabilities=True, min_topic_size=2)\n",
        "topics, probs = topic_model.fit_transform(df['Content_lemmatized'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QboNxjDVNCrM"
      },
      "outputs": [],
      "source": [
        "topic_model.get_topic_info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1V7UqubdNCrM"
      },
      "outputs": [],
      "source": [
        "# Get top 10 terms for a topic\n",
        "for i in range(5):\n",
        "  print(topic_model.get_topic(i))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}